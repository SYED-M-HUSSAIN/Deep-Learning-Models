# -*- coding: utf-8 -*-
"""SM_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eb1_w6Fj3UIJcuiOtZ8WYk7azKzDhM0n
"""

import matplotlib.pyplot as plt
import numpy as np
import PIL
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers import Activation,Dense,Flatten
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import os, json, math, librosa
import IPython.display as ipd
import librosa.display
import tensorflow as tf
import random
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint
#from google.colab import drive
from statistics import mode
#from google.colab import files
import time
import matplotlib.pyplot as plt
import numpy as np
import pathlib

# set configs

base_path = "/content/drive/MyDrive/faces"
target_size = (224,224,3)

# define shape for all images # get classes
classes = os.listdir(base_path)
print(classes)

from keras.preprocessing.image import ImageDataGenerator

batch_size = 32

datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=20,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   width_shift_range = 0.2,
                                   height_shift_range = 0.2,
                                   vertical_flip = True,
                                   validation_split=0.25)
train_gen = datagen.flow_from_directory(base_path,
                                               target_size=target_size[:2],
                                               batch_size=batch_size,
                                               class_mode='categorical',
                                               subset='training')
val_gen =  datagen.flow_from_directory(base_path,
                                               target_size=target_size[:2],
                                               batch_size=batch_size,
                                               class_mode='categorical',
                                               subset='validation',
                                               shuffle=False)

"""#MODEL 1"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, Dropout, Flatten, Dense
from tensorflow.keras.models import Model
# Import the VGG16 pretrained model
from tensorflow.keras.applications import VGG16

# initialize the model
vgg16 = VGG16(input_shape=(224,224,3), weights='imagenet', include_top=False)

# Freeze all but the last 3 layers
for layer in vgg16.layers[:-3]: layer.trainable = False

# build model
input = vgg16.layers[-1].output # input is the last output from vgg16

x = Dropout(0.25)(input)
x = Flatten()(x)
x = Dense(units=128, activation='relu')(x)
x = Dense(units=64, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# create the model
vgg16_model = Model(vgg16.input, output, name='VGG16_Model')
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

# Compile model
vgg16_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Initialize callbacks
reduceLR = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, mode='min', factor=0.2, min_lr=1e-6)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)

checkpoint = ModelCheckpoint('VGG16Model.weights.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')

callbacks = [reduceLR, early_stopping, checkpoint]
epochs = 50

# train model
HT1=vgg16_model.fit(train_gen, validation_data=val_gen,  epochs= epochs, callbacks= callbacks)
# Evaluate the model
vgg16_model.evaluate(val_gen)
from sklearn.metrics import classification_report

# Evaluate the model
val_pred = vgg16_model.predict(val_gen)

# Get predicted labels
val_pred_labels = np.argmax(val_pred, axis=1)

# Get true labels
val_true_labels = val_gen.classes

# Generate classification report
classification_rep = classification_report(val_true_labels, val_pred_labels,target_names=classes)
print(classification_rep)

"""#Model 2"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dropout, Flatten, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.applications import ResNet50

# Initialize the ResNet50 pretrained model
resnet50 = ResNet50(input_shape=(224, 224, 3), weights='imagenet', include_top=False)

# Freeze all layers
for layer in resnet50.layers[:-3]:
    layer.trainable = False

# Build model
input = resnet50.output

x = Dropout(0.25)(input)
x = Flatten()(x)
x = Dense(units=128, activation='relu')(x)
x = Dense(units=64, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Create the model
resnet50_model = Model(resnet50.input, output, name='ResNet50_Model')

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

# Compile model
resnet50_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Initialize callbacks
reduceLR = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, mode='min', factor=0.2, min_lr=1e-6)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)
checkpoint = ModelCheckpoint('ResNet50Model.weights.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')

callbacks = [reduceLR, early_stopping, checkpoint]
epochs = 50

# Train model
HT2= resnet50_model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=callbacks)

# Evaluate the model
resnet50_model.evaluate(val_gen)

from sklearn.metrics import classification_report

# Evaluate the model
val_pred = resnet50_model.predict(val_gen)

# Get predicted labels
val_pred_labels = np.argmax(val_pred, axis=1)

# Get true labels
val_true_labels = val_gen.classes

# Generate classification report
classification_rep = classification_report(val_true_labels, val_pred_labels, target_names=classes)
print(classification_rep)

"""#Model 3"""

# Import the MobileNet pretrained model
from tensorflow.keras.applications import MobileNet
# initializing the mobilenet model
mobilenet = MobileNet(input_shape=(224,224,3), weights='imagenet', include_top=False)

# freezing all but the last 5 layers
for layer in mobilenet.layers[:-5]:
  layer.trainable = False

# add few mor layers
x = mobilenet.layers[-1].output
x = Dropout(0.5)(x)
x = Flatten()(x)
x = Dense(32, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(16, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Create the model
mobilenet_model = Model(mobilenet.input, output, name= "Mobilenet_Model")

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

# Compile model
mobilenet_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Initialize callbacks
reduceLR = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, mode='min', factor=0.2, min_lr=1e-6)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)

checkpoint = ModelCheckpoint('Mobilenet_Model.weights.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')

callbacks = [reduceLR, early_stopping, checkpoint]
epochs = 50

# train model
HT3=mobilenet_model.fit(train_gen, validation_data=val_gen,  epochs= epochs, callbacks= callbacks)
# Evaluate the model
mobilenet_model.evaluate(val_gen)
from sklearn.metrics import classification_report

# Evaluate the model
val_pred = mobilenet_model.predict(val_gen)

# Get predicted labels
val_pred_labels = np.argmax(val_pred, axis=1)

# Get true labels
val_true_labels = val_gen.classes

# Generate classification report
classification_rep = classification_report(val_true_labels, val_pred_labels,target_names=classes)
print(classification_rep)

# plot confusion matrix
#plot_confusion_matrix(val_true_labels, val_pred_labels)

"""#Model 4"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dropout, Flatten, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.applications import InceptionV3

# Initialize the InceptionV3 pretrained model
inception_v3 = InceptionV3(input_shape=(224, 224, 3), weights='imagenet', include_top=False)

# Freeze all layers
for layer in inception_v3.layers[:-3]:
    layer.trainable = False

# Build model
input = inception_v3.output

x = Dropout(0.25)(input)
x = Flatten()(x)
x = Dense(units=128, activation='relu')(x)
x = Dense(units=64, activation='relu')(x)
output = Dense(2, activation='softmax')(x)

# Create the model
inception_v3_model = Model(inception_v3.input, output, name='InceptionV3_Model')

from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint

# Compile model
inception_v3_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Initialize callbacks
reduceLR = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, mode='min', factor=0.2, min_lr=1e-6)
early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)
checkpoint = ModelCheckpoint('InceptionV3Model.weights.hdf5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')

callbacks = [reduceLR, early_stopping, checkpoint]
epochs = 50

# Train model
HT4= inception_v3_model.fit(train_gen, validation_data=val_gen, epochs=epochs, callbacks=callbacks)

# Evaluate the model
inception_v3_model.evaluate(val_gen)

from sklearn.metrics import classification_report

# Evaluate the model
val_pred = inception_v3_model.predict(val_gen)

# Get predicted labels
val_pred_labels = np.argmax(val_pred, axis=1)

# Get true labels
val_true_labels = val_gen.classes

# Generate classification report
classification_rep = classification_report(val_true_labels, val_pred_labels, target_names=classes)
print(classification_rep)

"""#PLOT"""

import numpy as np
import matplotlib.pyplot as plt

def plot_metric(metric_values, labels, plot_name):
    '''
    This function will plot the accuracy of multiple models in a single graph.
    Args:
        metric_values: List of accuracy values for each model.
        labels: List of labels for each model.
        plot_name: The title of the graph.
    '''

    # Find the maximum number of epochs across all models
    max_epochs = max(len(metric) for metric in metric_values)

    # Interpolate the accuracy values for each model to align with the maximum number of epochs
    interpolated_values = []
    for metric in metric_values:
        interpolated_metric = np.interp(np.arange(max_epochs), np.arange(len(metric)), metric)
        interpolated_values.append(interpolated_metric)

    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.
    epochs = np.arange(max_epochs)

    # Plot the accuracy values for each model
    for i in range(len(interpolated_values)):
        plt.plot(epochs, interpolated_values[i], label=labels[i])

    # Add title to the plot.
    plt.title(plot_name)

    # Add legend to the plot.
    plt.legend()

    # Show the plot
    plt.show()

m1 = HT1.history['accuracy']
m2 = HT2.history['accuracy']
m3 = HT3.history['accuracy']
m4 = HT4.history['accuracy']
metric_values=[m1,m2,m3,m4]
labels = ['VGG16', 'ResNet50', 'MobileNet',  'Inception V3']
plot_name = 'Training Accuracy Comparison'

plot_metric(metric_values, labels, plot_name)

m1 = HT1.history['val_accuracy']
m2 = HT2.history['val_accuracy']
m3 = HT3.history['val_accuracy']
m4 = HT4.history['val_accuracy']
metric_values=[m1,m2,m3,m4]
labels = ['VGG16', 'ResNet50', 'MobileNet',  'Inception V3']
plot_name = 'Validation Accuracy Comparison'

plot_metric(metric_values, labels, plot_name)

m1 = HT1.history['loss']
m2 = HT2.history['loss']
m3 = HT3.history['loss']
m4 = HT4.history['loss']
metric_values=[m1,m2,m3,m4]
labels = ['VGG16', 'ResNet50', 'MobileNet',  'Inception V3']
plot_name = 'Training Loss Comparison'

plot_metric(metric_values, labels, plot_name)

m1 = HT1.history['val_loss']
m2 = HT2.history['val_loss']
m3 = HT3.history['val_loss']
m4 = HT4.history['val_loss']
metric_values=[m1,m2,m3,m4]
labels = ['VGG16', 'ResNet50', 'MobileNet',  'Inception V3']
plot_name = 'Validation Loss Comparison'

plot_metric(metric_values, labels, plot_name)